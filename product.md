# Oh, the reports you will write!

Imagine we work for a company that provides a checkout system for many online stores. In order to make smart business decisions, the analysts at our company need detailed reports on purchases.

When a customer completes a purchase, an event is generated by the checkout system containing the following information:
* Date: ISO 8601 instant; e.g. 2011-12-03T10:15:30Z
* Amount: total amount of purchase in cents; e.g. 4285 (€42.85)
* Payment method: Pay Now (credit card or bank transfer), Pay Later (invoice at the end of the month), or Slice It (pay in convenient monthly installments)
* Merchant ID: unique identifier of merchant; e.g. 1bb53ed1-787b-4543-9def-ea18eef7902e

Our business people want the following reports:
* Number of purchases per hour, broken down by amount bracket (less than €10, €10 - €50, €50-€100, €100-€500, more than €500)
* Number of purchases per hour, broken down by amount bracket and payment method
* Number of purchases, broken down by amount bracket and payment method
* Number of purchases per day, broken down by merchant
* Number of purchases, broken down by merchant and payment method

When a purchase is completed, the checkout system should publish an event, which our system needs to pick and aggregate. The checkout system is already posting these events to a logging service via HTTP, so we should stick to the existing data format in order to minimise work for the very busy checkout team.

An event is a JSON object:

```json
{
  "date": "2011-12-03T10:15:30Z",
  "amount": 4285,
  "paymentMethod": "SLICE_IT",
  "merchantId": "1bb53ed1-787b-4543-9def-ea18eef7902e"
}
```

We can decide where the checkout system should publish these events.

Our system should turn this into a list of aggregates for each report that needs to be generated and store the aggregates somewhere.

For example, given the a batch containing only the event above, we should generate an aggregate datapoint for the purchases by hour, amount bracket, and payment method report like this: `2011-12-03:10|10-50|SLICE_IT`. We need to sum all events for the same datapoint and turn them into an aggregate.

Our company already has a report visualisation frontend, so we should serve up our aggregates in the data format they already use, to minimise work for them. We can instruct them where to get the data.

Their aggregate is a JSON object like this:

```json
{
  "datapoint": "2011-12-03:10|10-50|SLICE_IT",
  "events": 42
}
```

## What to do?

Let's break this down into component parts. Our system needs to:
- Accept events from the checkout system
- Aggregate those events
- Store the aggregates
- Make the aggregates available for pulling by the report UI

We can start out with an ideal design, then adapt it to real-world constraints.

### Accepting events

We decide that the checkout system should publish events to a queue, from which we will consume. Being AWS aficionados, we know that the simplest queue of them all is SQS, the Simple Queue Service. It's fully managed, so all we have to do is create it and use it.

### Aggregating events

For consuming events from the queue and aggregating them, we choose AWS Lambda, which we've heard is really good at batch processing. We know that it can be automatically invoked when messages arrive on the queue, and better yet, have the messages automatically stuffed into the argument to our Lambda function, thus alleviating our need to explicitly read messages from the queue!

### Storing aggregates

Given the fact that the reporting UI will request aggregates, we decide that a key/value store is a perfect fit. Luckily, AWS has one of those: [DynamoDB](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html), which is a managed NoSQL database.

### Serving aggregates

As DynamoDB has an API, we decide that we'll just create an IAM role and policy that allows the reporting UI to read from our table and give the credentials to the reporting team.

Now that we've got an elegant, bulletproof design, let's get to building it!
